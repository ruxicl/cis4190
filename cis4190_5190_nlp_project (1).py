# -*- coding: utf-8 -*-
"""CIS4190/5190_NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xO1cpjayqhzjjHhM6GkWakibvpWUuatn
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import nltk
import os

!pip install --upgrade --no-cache-dir gdown
if not os.path.exists("Reviews.csv"):
    !gdown 1_kLSwiRYtiXF7h9V1FlTqTapOHiYU5Mk

if not os.path.exists("glove.840B.300d.txt"):
  !wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip
  !unzip glove.840B.300d.zip

# Remeber to change the path here for the corresponding files you need
df = pd.read_csv('Reviews.csv')
print(df.shape)
df_subset = df.head(1000)
print(df_subset.shape)

df_subset.head()

df['Score'].value_counts().sort_index().plot(kind='bar',
                                             title='Reviews Count',
                                             figsize=(12,8)).set_xlabel('Stars')

from sklearn.model_selection import train_test_split

#NLTK
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
from sklearn.metrics import precision_recall_fscore_support

sample_df = df.sample(frac=1, random_state=1)

# NLTK’s Pre-Trained Sentiment Analyzer on Text
sia = SentimentIntensityAnalyzer()
text_res = {}
y_data = []
for i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):
    myid = row['Id']
    text_res[myid] = sia.polarity_scores(row['Text'])
    y_data.append( int( round( row['Score'])))

y_text_test = pd.DataFrame(y_data)

# NLTK’s Pre-Trained Sentiment Analyzer on Summary
smry_df = sample_df[sample_df['Summary'].notna()]

sia = SentimentIntensityAnalyzer()
smry_res = {}
y_data = []
for i, row in tqdm(smry_df.iterrows(), total=len(smry_df)):
    myid = row['Id']
    smry_res[myid] = sia.polarity_scores(row['Summary'])
    y_data.append( int( round( row['Score'])))

y_smry_test = pd.DataFrame(y_data)

#NLTK Evaluations
from sklearn.metrics import f1_score

# Text accuracy
text_pred = []
for rows in text_res:
  text_pred.append( int( round( ((text_res[rows]['compound']+1))*2.5, 0) ) )

y_text = pd.DataFrame(text_pred)



# Summary accuracy
smry_pred = []
for rows in smry_res:
  smry_pred.append( int( round( ((smry_res[rows]['compound']+1))*2.5, 0) ) )

y_smry = pd.DataFrame(smry_pred)

print("Text accuracy:", np.mean(y_text_test == y_text))
print("Summary accuracy", np.mean(y_smry_test == y_smry))

f1_text = f1_score(y_text_test, y_text, average='weighted')
f1_smry = f1_score(y_smry_test, y_smry, average='weighted')

print("Text f-score:", f1_text)
print("Summary f-score:", f1_smry)

# Balanced dataset

df_2 = df.loc[df['Score'] == 2]
n_values = df_2.shape[0]
df_1 = df.loc[df['Score'] == 1].sample(n=n_values)
df_3 = df.loc[df['Score'] == 3].sample(n=n_values)
df_4 = df.loc[df['Score'] == 4].sample(n=n_values)
df_5 = df.loc[df['Score'] == 5].sample(n=n_values)

balance_df = pd.concat([df_1, df_2, df_3, df_4, df_5])

print(balance_df.shape)

#Shifted dataset

train_1 = df_1.sample(frac=0.7, random_state=1) 
train_2 = df_2.sample(frac=0.7, random_state=1) 

train_4 = df_4.sample(frac=0.3, random_state=1)
train_5 = df_5.sample(frac=0.3, random_state=1)

test_1 = df_1.merge(train_1, on=['Id'], 
                   how='left', indicator=True)
test_1 = test_1[test_1['_merge']=='left_only']

test_2 = df_2.merge(train_2, on=['Id'], 
                   how='left', indicator=True)
test_2 = test_2[test_2['_merge']=='left_only']

test_4 = df_4.merge(train_4, on=['Id'], 
                   how='left', indicator=True)
test_4 = test_4[test_4['_merge']=='left_only']

test_5 = df_5.merge(train_5, on=['Id'], 
                   how='left', indicator=True)
test_5 = test_5[test_5['_merge']=='left_only']

train_shift = pd.concat([train_1, train_2, train_4, train_5])

test_shift = pd.concat([test_1, test_2, test_4, test_5])

test_shift.drop(columns = ['ProductId_y',	'UserId_y',	'ProfileName_y', 'HelpfulnessNumerator_y',	'HelpfulnessDenominator_y',	'Score_y',	'Time_y',	'Summary_y',	'Text_y',	'_merge'], axis=1, inplace=True)

# test_shift.rename(columns = ['ProductId_x' : 'ProductId',	'UserId_x' : 'UserId' ,	'ProfileName_x' : 'ProfileName', 'HelpfulnessNumerator_x' : 'HelpfulnessNumerator',	'HelpfulnessDenominator_x' : 'HelpfulnessDenominator',	'Score_x' : 'Score',	'Time_x' : 'Time',	'Summary_x' : 'Summary',	'Text_x' : 'Text'], axis=1, inplace=True)
test_shift.rename(columns={'ProductId_x': 'ProductId', 'UserId_x': 'UserId', 'ProfileName_x': 'ProfileName', 'HelpfulnessNumerator_x': 'HelpfulnessNumerator', 'HelpfulnessDenominator_x': 'HelpfulnessDenominator', 'Score_x': 'Score', 'Time_x': 'Time', 'Summary_x': 'Summary', 'Text_x': 'Text'}, inplace=True)

print(test_shift.shape)
print(train_shift.shape)

# Baseline 1: logistic regression with TF-IDF
# Multiclass classfication - predict a score 1 - 5

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# set the flag on for dataset shift
extreme_dataset_shift_on = False

# set to either 'Text' or 'Summary'
used_feature = 'Summary'

# For dataset shift, we will train the model on 4, 5 - star reviews and we will evaluate it on 1, 2 - star reviews
if extreme_dataset_shift_on:
  shifted_train_df = df.loc[df['Score'].isin([4, 5])]
  shifted_test_df = df.loc[df['Score'].isin([1, 2])]

  X_train = shifted_train_df[used_feature]
  y_train = shifted_train_df['Score']

  X_test = shifted_test_df[used_feature]
  y_test = shifted_test_df['Score']
else:
  # select features and the label
  updated_balance_df = balance_df[balance_df[used_feature].notna()]
  X = updated_balance_df[used_feature]
  y = updated_balance_df['Score']

  # train - test split (maybe add validation too)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)


# TF-IDF
# Tf-idf(w, d)= BoW(w, d) * log(# of reviews / # of reviews containing the word w)
#  where BoW(w, d) = # of times word w appears in review d

# tune params (esp. ngram_range, lowercase, max_features) (test also ngram_range=(1, 2))
transformer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=150000)
X_train_tf_idf = transformer.fit_transform(X_train)
X_test_tf_idf = transformer.transform(X_test)

# Logistic regression
# to do: tune the params (esp. - suggestion: C=5e1, n_jobs = 4, random_state)
log_regression = LogisticRegression( solver='lbfgs', multi_class='multinomial')
log_regression.fit(X_train_tf_idf, y_train)
y_predict_baseline1 = log_regression.predict(X_test_tf_idf)


f1 = f1_score(y_test, y_predict_baseline1, average='weighted')
# f-1 scores
print(f"f1 score obtained with dataset_shift_on{extreme_dataset_shift_on}, and using the {used_feature} column: {f1}")

# Baseline 2: Random prediction
y_predict_baseline2 = np.random.randint(low = 1, high = 6, size = len(X_test))

# Baseline 3: Constant prediction
y_predict_baseline3 = np.full(len(X_test), fill_value = 5)

!pip install transformers

# Baseline 4: BERT
from transformers import BertTokenizer, BertModel, AutoTokenizer, BertForPreTraining
import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score

class BertSentimentClassifier(nn.Module):
    def __init__(self, num_classes):
        super(BertSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(768, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs['pooler_output']
        x = self.dropout(pooled_output)
        logits = self.fc(x)
        return logits

used_feature = 'Text'

small_df = balance_df[balance_df[used_feature].notna()]
small_df = small_df.sample(n=int(len(small_df) * 0.1))
X = small_df[used_feature]
y = small_df['Score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertSentimentClassifier(num_classes=5)

train_tokens = tokenizer.batch_encode_plus(X_train.tolist(), max_length=30, truncation=True, padding='max_length',
                                           return_tensors='pt')
test_tokens = tokenizer.batch_encode_plus(X_test.tolist(), max_length=30, truncation=True, padding='max_length',
                                          return_tensors='pt')

train_input_ids = train_tokens['input_ids']
train_attention_mask = train_tokens['attention_mask']
test_input_ids = test_tokens['input_ids']
test_attention_mask = test_tokens['attention_mask']

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
loss_fn = nn.CrossEntropyLoss()
num_epochs = 3

for epoch in range(num_epochs):
    optimizer.zero_grad()
    train_logits = model(train_input_ids, train_attention_mask)
    train_loss = loss_fn(train_logits, y_train)
    train_loss.backward()
    optimizer.step()

    train_probs = torch.softmax(train_logits, dim=1)
    train_preds = torch.argmax(train_probs, dim=1)

    train_accuracy = accuracy_score(y_train, train_preds)

    test_logits = model(test_input_ids, test_attention_mask)
    test_probs = torch.softmax(test_logits, dim=1)
    test_preds = torch.argmax(test_probs, dim=1)

"""# GLOVE + LSTM"""

# HYPERPARAMETERS AND CONSTANTS FOR THE LSTM MODEL
import torch
import torch.nn as nn

DATASET_SHIFT = False
INPUT = "Text" # "Text" or "Summary"
FRAC_SAMPLES = 0.1 # fraction of samples to use
EPOCHS = 10
DIM_OUTPUT = 5  # number of classes / scores (1-5)
BATCH_SIZE = 256

dev_every = 1000
save_path = "best_model"

GLOVE_FILE = "content.nosync/glove.840B.300d.txt"
d_embed = 300
LSTM_DF = balance_df.sample(frac=FRAC_SAMPLES)
LSTM_SHIFTED_TRAIN_DF = train_shift.sample(frac=FRAC_SAMPLES)
LSTM_SHIFTED_TEST_DF = test_shift.sample(frac=FRAC_SAMPLES)

PADDING_VALUE = 0 # The value used for padding the reviews to the same length

def get_dataset(dataset_shift="False", input="Text"):
    """
    Return the training and test datasets
    :param dataset_shift_on: whether to use dataset shift (train is from sets 4,5 and test is from sets 1,2)
    :param input: the input to the model ("Text" or "Summary")
    :param sample: the number of samples to use
    :return: X_train, X_test, y_train, y_test. X = text of the review, y = score of the review
    """
    # For dataset shift, we will train the model on 4, 5 - star reviews and we will evaluate it on 1, 2 - star reviews
    if dataset_shift:
        X_train = LSTM_SHIFTED_TRAIN_DF[input]
        y_train = LSTM_SHIFTED_TRAIN_DF['Score']

        X_test = LSTM_SHIFTED_TEST_DF[input]
        y_test = LSTM_SHIFTED_TEST_DF['Score']
    else:
        # select features and the label
        X = LSTM_DF[input]
        y = LSTM_DF['Score']

        # train - test split (maybe add validation too)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

    return X_train, X_test, y_train, y_test

# Given a text review, find the embedding for each word in the review
def get_embedding(text, vocab):
    text = text.lower()
    text = text.split()
    embedding = np.zeros(300)
    for word in text:
        try:
            embedding += vocab[word]
        except:
            pass
    return embedding 

# Given a set of reviews, find the embedding for each review
def get_all_embeddings(reviews, vocab):
    embeddings = []
    for review in tqdm.tqdm(reviews):
        embeddings.append(get_embedding(review, vocab))
    return embeddings

# Get the vocabulary to use for the embeddings
# Only use the embedding for the words in the training dataset
import string


def get_vocab(X_train):
    # Merge all the training reviews into one string. Make it all lowercase
    all_training_text = " ".join(X_train.to_list()).lower()

    # Remove punctuation
    all_training_text = all_training_text.translate(str.maketrans('', '', string.punctuation))

    # Get the set of unique words used in the training set
    return sorted(set(all_training_text.split()))

X_train, X_test, y_train, y_test = get_dataset(dataset_shift=DATASET_SHIFT, input=INPUT)
vocab_set = get_vocab(X_train)
EXTRA_WORD = len(vocab_set)

# Inspired by CIS4190 HW6 and https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
# The glove dictionary should only have words that are present in the train vocabulary.
import tqdm


def get_glove_vocab(vocab_set):
    glove_map = {}
    ignored = 0 # How many words we ignored as they weren't in the training dataset
    with open(GLOVE_FILE, 'r') as f:
        lines = f.readlines()
        for line in tqdm.tqdm(lines):
            values = line.split()
            word = values[0].lower()
            try:

                if word in vocab_set:
                  vector = np.asarray(values[1:], "float32")
                  glove_map[word] = vector
            except:
                pass
    return glove_map
vocabulary = get_glove_vocab(vocab_set)

# As we did in HW 6, create a weight matrix that maps from word id to embedding

def get_weight_matrix(n_embed, d_embed, glove_map):
    """
    Initialize the weight matrix

    INPUT:
    n_embed         - size of the dictionary of embeddings
    d_embed         - the size of each embedding vector

    OUTPUT:
    weights_matrix  - matrix of mapping from word id to embedding

    """

    weights_matrix = np.zeros((n_embed + 1, d_embed))

    for i in range(n_embed):
        i_th_word = vocab_set[i]

        if i_th_word in glove_map:
            weights_matrix[i] = glove_map[i_th_word]
        else:
            weights_matrix[i] = np.random.random(d_embed)

        weights_matrix[EXTRA_WORD] = np.zeros(d_embed)

    return weights_matrix

n_embed = len(vocab_set)
weights_matrix = get_weight_matrix(n_embed, d_embed, vocabulary)

# Adopted from HW 6
def create_emb_layer(weights_matrix, non_trainable=False):
    """
    Create the embedding layer

    INPUT:
    weights_matrix  - matrix of mapping from word id to embedding
    non_trainable   - Flag for whether the weight matrix should be trained.
                      If it is set to True, don't update the gradients

    OUTPUT:
    emb_layer       - embedding layer

    """

    emb_layer = nn.Embedding(weights_matrix.shape[0], weights_matrix.shape[1])
    emb_layer.from_pretrained(torch.tensor(weights_matrix), freeze=False)

    if non_trainable:
        # If non_trainable is set to True, don't update the gradients
        emb_layer.weights.requires_grad = False

    return emb_layer

# Crate an LSTM model to take reviews, create an embedding of them and then output a score
# between 1 and 5
class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(LSTM, self).__init__()

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.embed = create_emb_layer(weights_matrix, False)
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.hidden = None

    # forward layer of a classical lstm
    def forward(self, word_indices):
        """
        :param word_indices: indices of the words in the review
        :return: score of the review
        """

        batch, max_sent = word_indices.size()

        # Create a mask for all the items that were padded (i.e. not part of the review but added to make all reviews the same length)
        mask = word_indices != PADDING_VALUE
        mask = mask.reshape(batch, max_sent, 1)
        mask = mask.expand(batch, max_sent, d_embed)

        embeddings = self.embed(word_indices)

        masked_input = embeddings * mask

        # Pass the inputs through the LSTM layer and then a forward layer to get the output (Score)
        lstm_out, _ = self.lstm(masked_input)
        output = self.fc(lstm_out[:, -1, :])  # Classification layer

        return output

from torch.utils.data import Dataset


# Create a Dataset for the LSTM that will return a review and its score
class LSTM_Dataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y - 1 # Need to subtract 1 since the scores predicted by the LSTM are zero indexed

    def __getitem__(self, index):
        return self.X.iloc[index], self.y.iloc[index]

    def __len__(self):
        return len(self.y)

from sklearn.metrics import f1_score


def evaluate(loader, model, device, split = "valid"):
    model.eval()
    n_correct, n = 0, 0
    losses = []
    y_pred = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(loader):

            data, label = batch
            # Convert the words with the indices of where the embedding is stored in the weights' matrix.
            # we drop words not in the vocabulary / training set
            data = list(map(lambda x: torch.tensor([vocab_set.index(word) if word in vocab_set else EXTRA_WORD for word in x ]), data))

            # Pad the reviews to the same length in the batch
            data = pack_sequence(data, enforce_sorted=False)
            data, _ = pad_packed_sequence(data, batch_first=True, padding_value=PADDING_VALUE)

            data = data.to(device)
            label = label.to(device).long()
            predictions = model(data)

            if split != "test":
                n_correct += (torch.max(predictions, 1)[1].view(label.size()) == label).sum().item()
                n += predictions.shape[0]
                loss = criterion(predictions, label)
                losses.append(loss.data.cpu().numpy())
            else:
                y_pred.extend(torch.max(predictions, 1)[1].view(label.size()).tolist())
    if split != "test":
        acc = 100. * n_correct / n
        loss = np.mean(losses)
        score_predicted = torch.argmax(predictions, dim=1)
        f1 = f1_score(label, score_predicted, average='weighted')

        return acc, loss, f1
    else:
        return y_pred

import time
# LSTM Training loop
from torch import optim

from torch.utils.data import DataLoader

from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence

criterion = nn.CrossEntropyLoss()

def train_lstm(lr=0.001, batch_size=BATCH_SIZE, weight_decay=1e-5):
    accuracies = []
    # Load the training, validation and test datasets for the LSTM

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    X_train, X_test, y_train, y_test = get_dataset(dataset_shift=DATASET_SHIFT, input=INPUT)

    # Keep 80 % of the training data for training and 20% for validation
    valid_start_idx = int(0.8 * len(X_train))

    train_data = LSTM_Dataset(X_train[:valid_start_idx], y_train[:valid_start_idx])
    valid_data = LSTM_Dataset(X_train[valid_start_idx:], y_train[valid_start_idx:])
    test_data = LSTM_Dataset(X_train, y_train)

    # Create the training, validation and test sets as torch Datasets
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)

    model = LSTM(input_dim=d_embed, hidden_dim=100, output_dim=DIM_OUTPUT).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    acc, val_loss, f1 = evaluate(valid_loader, model, device)
    best_acc = acc

    print(
        'epoch |   %        |  loss  |  avg   |val loss|   acc   | f1    |  best  | time | save |')

    iterations = 0
    start = time.time()
    _save_ckp = ''

    for epoch in range(EPOCHS):
        # train_iter.init_epoch()
        n_correct, n_total, train_loss = 0, 0, 0
        last_val_iter = 0

        batch_accuracies = []
        for batch_idx, batch in enumerate(train_loader):

            data, label = batch
            # Convert the words with the indices of where the embedding is stored in the weights' matrix.
            data = list(map(lambda x: torch.tensor([vocab_set.index(word) if word in vocab_set else EXTRA_WORD for word in x ]), data))

            # Pad the reviews to the same length in the batch
            data = pack_sequence(data, enforce_sorted=False)
            data, lengths = pad_packed_sequence(data, batch_first=True, padding_value=PADDING_VALUE)

            # switch model to training mode, clear gradient accumulators
            model.train()
            optimizer.zero_grad()

            iterations += 1

            data = data.to(device)
            label = label.to(device).long()

            # Take the most probable score to be the prediction
            predictions = model(data)

            loss = criterion(predictions, label)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            print('\r {:4d} | {:4d}/{} | {:.4f} | {:.4f} |'.format(
                epoch, batch_size * (batch_idx + 1), len(train_data), loss.item(),
                       train_loss / (iterations - last_val_iter)), end='')

            if iterations > 0:
                acc, val_loss, f1 = evaluate(valid_loader, model, device)
                batch_accuracies.append(acc)

                if acc > best_acc:
                    best_acc = acc
                    torch.save(model.state_dict(), save_path)
                    _save_ckp = '*'

                print(
                    ' {:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} |'.format(
                        val_loss, acc, f1, best_acc, (time.time() - start) / 60,
                        _save_ckp))

                train_loss = 0
                last_val_iter = iterations

        accuracies.append(np.mean(batch_accuracies))

    model.load_state_dict(torch.load(save_path)) #this will be the best model
    test_y_pred = evaluate(test_loader,model, device,"test")
    print("\nValidation Accuracy : ", evaluate(valid_loader,model, device))
    return best_acc, test_y_pred, accuracies

best_acc, test_y_pred, accuracies = train_lstm()